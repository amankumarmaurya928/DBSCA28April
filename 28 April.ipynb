{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2068549-db82-42a6-91b7-fdb6ceb5bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into\\n   groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the\\n   objects within each cluster are broadly similar to each other.\\n   In conclusion, the main differences between Hierarchical and Partitional Clustering are that each cluster starts as \\n   individual clusters or singletons. With every iteration, the closest clusters get merged. This process repeats until one\\n   single cluster remains for Hierarchical clustering.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into\n",
    "   groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the\n",
    "   objects within each cluster are broadly similar to each other.\n",
    "   In conclusion, the main differences between Hierarchical and Partitional Clustering are that each cluster starts as \n",
    "   individual clusters or singletons. With every iteration, the closest clusters get merged. This process repeats until one\n",
    "   single cluster remains for Hierarchical clustering.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0b7eee-c8e4-4987-b86a-bb11716bfd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are two types of hierarchical clustering: divisive (top-down) and agglomerative (bottom-up).\\n   There are two main types of hierarchical clustering:\\n1. Agglomerative: Initially, each object is considered to be its own cluster. According to a particular procedure, the\\n   clusters are then merged step by step until a single cluster remains. \\n2. Divisive: The Divisive method is the opposite of the Agglomerative method.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''There are two types of hierarchical clustering: divisive (top-down) and agglomerative (bottom-up).\n",
    "   There are two main types of hierarchical clustering:\n",
    "1. Agglomerative: Initially, each object is considered to be its own cluster. According to a particular procedure, the\n",
    "   clusters are then merged step by step until a single cluster remains. \n",
    "2. Divisive: The Divisive method is the opposite of the Agglomerative method.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51820c53-c38b-478f-8043-62d91b00a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the \\n   square root of the sum of the square differences. However, for gene expression, correlation distance is often used.\\n   In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs \\n   of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all\\n   pairwise distances between cluster r and cluster s.\\n   All three methods, i.e., single link, complete link, and average link, can be used for finding dissimilarity between two \\n   clusters in hierarchical clustering( can be found in the Python library scikit-learn).\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the \n",
    "   square root of the sum of the square differences. However, for gene expression, correlation distance is often used.\n",
    "   In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs \n",
    "   of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all\n",
    "   pairwise distances between cluster r and cluster s.\n",
    "   All three methods, i.e., single link, complete link, and average link, can be used for finding dissimilarity between two \n",
    "   clusters in hierarchical clustering( can be found in the Python library scikit-learn).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb81ec02-96fa-4104-95d9-b21098ed1363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that \\n   shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join them in a graph \\n   and the height of the join will be the distance between those clusters.\\n   The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by\\n   simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K.\\n   Finding the optimal number of clusters is an important part of this algorithm. A commonly used method for finding the\\n   optimum K value is Elbow Method.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that \n",
    "   shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join them in a graph \n",
    "   and the height of the join will be the distance between those clusters.\n",
    "   The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by\n",
    "   simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K.\n",
    "   Finding the optimal number of clusters is an important part of this algorithm. A commonly used method for finding the\n",
    "   optimum K value is Elbow Method.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e7810c-d348-4bca-b4f5-5143f469aefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dendrogram is a tree-like structure that explains the relationship between all the data points in the system. However, \\n   like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical\\n   direction (y-axis) in it represents the distance between clusters in some metric.\\n   A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an \\n   output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to \\n   clusters.\\n   The key to interpreting a hierarchical cluster analysis is to look at the point at which any given pair of cards “join \\n   together” in the tree diagram. Cards that join together sooner are more similar to each other than those that join \\n   together later.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''A dendrogram is a tree-like structure that explains the relationship between all the data points in the system. However, \n",
    "   like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical\n",
    "   direction (y-axis) in it represents the distance between clusters in some metric.\n",
    "   A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an \n",
    "   output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to \n",
    "   clusters.\n",
    "   The key to interpreting a hierarchical cluster analysis is to look at the point at which any given pair of cards “join \n",
    "   together” in the tree diagram. Cards that join together sooner are more similar to each other than those that join \n",
    "   together later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dd90db-f9ba-4cae-ab96-4768db3bf200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes of course, categorical data are frequently a subject of cluster analysis, especially hierarchical. A lot of proximity\\n   measures exist for binary variables (including dummy sets which are the litter of categorical variables); also entropy \\n   measures.\\n   For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the \\n   square root of the sum of the square differences. However, for gene expression, correlation distance is often used.\\n   k-modes defines clusters based on matching categories between the data points. The k-Prototype algorithm is an extension to\\n   the k-Modes algorithm that combines the k-modes and k-means algorithms and is able to cluster mixed numerical and \\n   categorical variables.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Yes of course, categorical data are frequently a subject of cluster analysis, especially hierarchical. A lot of proximity\n",
    "   measures exist for binary variables (including dummy sets which are the litter of categorical variables); also entropy \n",
    "   measures.\n",
    "   For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the \n",
    "   square root of the sum of the square differences. However, for gene expression, correlation distance is often used.\n",
    "   k-modes defines clusters based on matching categories between the data points. The k-Prototype algorithm is an extension to\n",
    "   the k-Modes algorithm that combines the k-modes and k-means algorithms and is able to cluster mixed numerical and \n",
    "   categorical variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6455247c-057d-43c1-be03-c9c91fba5026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In K-Means clustering outliers are found by distance based approach and cluster based approach. In case of hierarchical\\n   clustering, by using dendrogram outliers are found. The goal of the project is to detect the outlier and remove the outliers\\n   to make the clustering more reliable.\\n   Interquartile Range (IQR) is important because it is used to define the outliers.\\n   The single linkage hierarchical clustering algorithm has also been investigated by Arias-Castro (2011) and Auray et al.\\n   However, it is well known that this algorithm is sensitive to outliers.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''In K-Means clustering outliers are found by distance based approach and cluster based approach. In case of hierarchical\n",
    "   clustering, by using dendrogram outliers are found. The goal of the project is to detect the outlier and remove the outliers\n",
    "   to make the clustering more reliable.\n",
    "   Interquartile Range (IQR) is important because it is used to define the outliers.\n",
    "   The single linkage hierarchical clustering algorithm has also been investigated by Arias-Castro (2011) and Auray et al.\n",
    "   However, it is well known that this algorithm is sensitive to outliers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9446ea6-09a8-43d7-bcdb-ee44a07a10f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
